{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9613e11f",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Création d'un chatbot avec MemN2N utilisant le mécanisme de l’Attention</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc6e333",
   "metadata": {},
   "source": [
    "# I) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16015ab",
   "metadata": {},
   "source": [
    "<p>Dans ce tutoriel, nous allons explorer comment créer un chatbot qui peut répondre à des questions en utilisant un modèle de réseau de mémoire end-to-end (MemN2N) avec le mécanisme d'attention. Le modèle sera entraîné sur un ensemble de données qui contient des histoires, des questions et des réponses « oui » ou « non » associées. Nous allons également expliquer comment prétraiter les données en séquences de mots encodées sous forme de nombres pour être traitées par le modèle.</p>\n",
    "\n",
    "<p>Le mécanisme d'attention est un composant clé de notre modèle de chatbot. Il permet au modèle de se concentrer sur les parties importantes des données d'entrée tout en ignorant les parties moins importantes. Nous allons discuter de la manière dont le mécanisme d'attention fonctionne et de son rôle dans la création d'un chatbot efficace.</p>\n",
    "\n",
    "<p>Nous allons également plonger dans MemN2N, un modèle de réseau de mémoire end-to-end qui utilise une approche d'apprentissage en mémoire pour résoudre des tâches de question-réponse. Nous expliquerons comment utiliser ce modèle pour construire notre chatbot.</p>\n",
    "\n",
    "<p>Pour ce faire, nous allons utiliser l'ensemble de données Babi de Facebook Research pour entraîner notre modèle. Cet ensemble de données comprend plusieurs histoires courtes qui contiennent des questions et des réponses. Nous allons prétraiter ces données pour les utiliser avec notre modèle de chatbot.</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c3d230",
   "metadata": {},
   "source": [
    "# II) Prétraitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c11904d",
   "metadata": {},
   "source": [
    "<p>On importe les librairies nécéssaires.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d19a641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importatation des librairies nécéssaires\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ccdebb",
   "metadata": {},
   "source": [
    "<p>Les lignes de code suivantes utilisent le module Python pickle pour charger des données prétraitées à partir de fichiers de texte dans les variables <code>train_data</code> et <code>test_data</code>.</p>\n",
    "\n",
    "<p>La première ligne ouvre le fichier \"train_qa.txt\" en mode binaire (\"rb\") et utilise le module pickle pour désérialiser les données dans le fichier. Les données désérialisées sont stockées dans la variable <code>train_data</code>. La deuxième ligne suit le même processus pour le fichier \"test_qa.txt\", stockant les données désérialisées dans la variable <code>test_data</code>.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a18c52ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_qa.txt\", \"rb\") as fp:   \n",
    "    train_data =  pickle.load(fp)\n",
    "    \n",
    "with open(\"test_qa.txt\", \"rb\") as fp:  \n",
    "    test_data =  pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872669ab",
   "metadata": {},
   "source": [
    "<p>Ici on crée un ensemble (<code>set</code>) qui contiendra tous les mots présents dans le dataset de données utilisé.</p>\n",
    "<ol>\n",
    "<li>On crée un ensemble vide appelé \"<code>vocab</code>\".</li>\n",
    "<li>On combine les données d'entraînement et de test en une seule variable appelée \"<code>all_data</code>\".</li>\n",
    "<li>Pour chaque histoire (<code>story</code>), question et réponse (<code>answer</code>) dans \"<code>all_data</code>\", on ajoute tous les mots uniques de l'histoire et de la question à l'ensemble \"<code>vocab</code>\".</li>\n",
    "<li>On ajoute les mots \"<code>no</code>\" et \"<code>yes</code>\" à l'ensemble \"<code>vocab</code>\" </li>\n",
    "<li>On calcule la longueur de l'ensemble \"<code>vocab</code>\" en ajoutant 1 pour inclure un index réservé pour les mots inconnus.</li>\n",
    "</ol>\n",
    "<p> Qu'est-ce qu'un \"<code>set</code>\" et \"<code>union</code>\" :</p>\n",
    "<ul>\n",
    "<li>Un \"<code>set</code>\" en Python est une structure de données qui stocke des éléments uniques et non ordonnés. Il est similaire à une liste ou un tuple, mais avec l'ajout que chaque élément est unique, c'est-à-dire qu'il n'y a pas de doublons dans un ensemble.</li>\n",
    "<li>La méthode \"<code>union</code>\" est utilisée pour combiner deux ensembles en un seul ensemble qui contient tous les éléments uniques de ces ensembles. En d'autres termes, la méthode \"<code>union</code>\" permet d'ajouter des éléments à un ensemble sans créer de doublons.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e4080b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée un ensemble (set) qui contiendra tout les mots dans notre dataset\n",
    "vocab = set()\n",
    "all_data = test_data + train_data\n",
    "\n",
    "for story, question , answer in all_data:\n",
    "\n",
    "    vocab = vocab.union(set(story))\n",
    "    vocab = vocab.union(set(question))\n",
    "    \n",
    "vocab.add('no')\n",
    "vocab.add('yes')   \n",
    "vocab_len = len(vocab) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ca4857",
   "metadata": {},
   "source": [
    "<p>Ici, on calcule la longueur maximale des histoires \"<code>story</code>\"  et des questions \"<code>questions</code>\"dans le dataset de données \"<code>all_data</code>\".</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619937ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_story_len = max([len(data[0]) for data in all_data])\n",
    "\n",
    "max_question_len = max([len(data[1]) for data in all_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9860f4d2",
   "metadata": {},
   "source": [
    "<p> On convertit chaque séquence de mots en une séquence d'entiers à l'aide de la classe \"<code>Tokenizer</code>\" et de la méthode \"<code>texts_to_sequences</code>\". Les données d'entraînement (histoires, questions et réponses) sont préparées pour l'entraînement du modèle de chatbot en vue de construire un modèle capable de comprendre les séquences de mots et de prédire des réponses adéquates.</p>\n",
    "<ol>\n",
    "<li>On crée une instance de la classe \"<code>Tokenizer</code>\" du module \"tensorflow.keras.preprocessing.text\" pour convertir chaque séquence de mots en une séquence d'entiers. On utilise l'argument \"filters\" avec une valeur vide pour indiquer au tokenizer de ne pas supprimer les caractères spéciaux par défaut.<br>\n",
    "<code>tokenizer = Tokenizer(filters=[])</code></li>\n",
    "<li>On utilise la méthode \"<code>fit_on_texts</code>\" du tokenizer pour adapter les données d'entraînement (\"vocab\") à notre tokenizer. Cette méthode transforme chaque mot unique en un index numérique.<br>\n",
    "<code>tokenizer.fit_on_texts(vocab)</code></li>\n",
    "<li>On crée trois listes vides : \"<code>train_story_text</code>\", \"<code>train_question_text</code>\" et \"<code>train_answers</code>\".<br>\n",
    "\n",
    "<li>On parcourt chaque donnée d'entraînement \"<code>train_data</code>\" et on extrait l'histoire, la question et la réponse de chaque donnée. On ajoute chaque histoire dans la liste \"<code>train_story_text</code>\", chaque question dans la liste \"<code>train_question_text</code>\", et chaque réponse dans la liste \"<code>train_answers</code>\".<br>\n",
    "\n",
    "\n",
    "<li>On utilise la méthode \"<code>texts_to_sequences</code>\" du tokenizer pour convertir chaque élément de la liste \"<code>train_story_text</code>\" en une séquence d'entiers correspondante. Cette méthode effectue les tâches suivantes : tokenization des mots, remplacement de chaque mot par son index dans le dictionnaire, et padding/troncature de chaque séquence pour qu'elles aient la même longueur.<br>\n",
    "<code>train_story_seq = tokenizer.texts_to_sequences(train_story_text)</code></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer(filters=[])\n",
    "tokenizer.fit_on_texts(vocab)\n",
    "\n",
    "train_story_text = []\n",
    "train_question_text = []\n",
    "train_answers = []\n",
    "\n",
    "for story,question,answer in train_data:\n",
    "    train_story_text.append(story)\n",
    "    train_question_text.append(question)\n",
    "    \n",
    "train_story_seq = tokenizer.texts_to_sequences(train_story_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e405ab2",
   "metadata": {},
   "source": [
    "<p>On va créer une fonction qui permet de transformer une phrase en une séquence de nombres en remplaçant chaque mot par son index dans le dictionnaire d'index de mots <code>tokenizer.word_index</code> , qui est fourni en entrée de la fonction. La fonction prend également en entrée deux paramètres qui correspondent à la longueur maximale de l'histoire et de la question. Ces paramètres sont utilisés pour ajouter des zéros à la fin de la séquence si elle est plus courte que la longueur maximale spécifiée. La fonction retourne donc une séquence de nombres (avec des zéros ajoutés si nécessaire) pour chaque phrase (histoire ou question) dans les données d'entrée. </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22df5b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stories_vectorization(data, word_index=tokenizer.word_index, max_story_len=max_story_len,max_question_len=max_question_len):    \n",
    "    \n",
    "    X = []\n",
    "    Xq = []\n",
    "    Y = []\n",
    "    \n",
    "    \n",
    "    for story, query, answer in data:\n",
    "        \n",
    "       \n",
    "        x = [word_index[word.lower()] for word in story] \n",
    "        xq = [word_index[word.lower()] for word in query]\n",
    "        y = np.zeros(len(word_index) + 1)\n",
    "        y[word_index[answer]] = 1\n",
    "        \n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "        \n",
    "    \n",
    "    return (pad_sequences(X, maxlen=max_story_len),pad_sequences(Xq, maxlen=max_question_len), np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9cf4aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ..., 19, 17, 15],\n",
       "       [ 0,  0,  0, ..., 19,  5, 15],\n",
       "       [ 0,  0,  0, ..., 19,  5, 15],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ..., 19, 27, 15],\n",
       "       [ 0,  0,  0, ..., 19,  5, 15],\n",
       "       [ 0,  0,  0, ..., 27, 20, 15]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_train, queries_train, answers_train = stories_vectorization(train_data)\n",
    "inputs_test, queries_test, answers_test = stories_vectorization(test_data)\n",
    "inputs_test\n",
    "tokenizer.word_index['yes']\n",
    "tokenizer.word_index['no']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7557da",
   "metadata": {},
   "source": [
    "# III) Entrainement du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df430ce",
   "metadata": {},
   "source": [
    "<p> Pour créer notre modèle, on va utiliser l'architecture <code>MemN2N</code> pour entraîner un chatbot. MemN2N est une architecture de réseau de neurones qui utilise une approche de mémoire à court terme pour répondre aux questions posées en utilisant une histoire donnée. Pour en savoir plus sur cette architecture, vous pouvez consulter l'article original : <a href=\"http://arxiv.org/abs/1503.08895\">End-To-End Memory Networks</a>. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31481da",
   "metadata": {},
   "source": [
    "<p>Ces deux lignes de code vont créer deux tenseurs d'entrée pour le modèle.</p>\n",
    "<p> <code>Input((max_story_len,))</code> ve créer un tenseur d'entrée pour les histoires, qui est une séquence d'entiers de longueur <code>max_story_len</code>.</p>\n",
    "<p><code>Input((max_question_len,))</code> va créer un tenseur d'entrée pour les questions, qui est une séquence d'entiers de longueur <code>max_question_len</code>.</p>\n",
    "<p>Ces tenseurs d'entrée sont nécessaires pour entraîner le modèle et sont utilisés pour fournir les données d'entrée (histoires et questions) au réseau de neurones lors de l'entraînement et de l'évaluation. Les tenseurs d'entrée définis par ces lignes de code sont passés en tant qu'arguments à la méthode <code>fit()</code> lors de l'entraînement du modèle.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5af577d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = Input((max_story_len,))\n",
    "question = Input((max_question_len,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d77852f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p>Dans l'architecture MemN2N, l'idée est de combiner l'histoire (story) et la question pour prédire la réponse. L'histoire peut être considérée comme une séquence de mots qui peut être encodée en une séquence de vecteurs denses. Cependant, le traitement de la question est différent car elle est plus courte que l'histoire et a une signification différente.</p>\n",
    "\n",
    "<p>\n",
    "    Dans l'architecture MemN2N, deux encodeurs sont utilisés pour représenter l'histoire et la question. L'encodeur <code>input_encoder_m</code> est utilisé pour encoder la séquence de l'histoire (<code>input_sequence</code>) en une séquence de vecteurs denses de taille fixe. L'encodeur <code>input_encoder_c</code> est également utilisé pour encoder la séquence de l'histoire (<code>input_sequence</code>), mais cette fois-ci pour encoder la séquence en une séquence de vecteurs denses de taille <code>max_question_len</code>. Cela permet de tenir compte de la question dans le calcul de la réponse.\n",
    "\n",
    "L'utilisation de ces deux encodeurs différents pour représenter l'histoire et la question permet d'avoir deux représentations distinctes pour chaque entrée, ce qui permet au modèle de tenir compte de la question lors de la prédiction de la réponse.\n",
    "  </p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p>Pour résumer,dans l'architecture MemN2N, il y a trois encodeurs qui sont utilisés pour représenter l'histoire, la question et la correspondance entre l'histoire et la question.</p>\n",
    "<ul>\n",
    "  <li><p>L'encodeur <code>input_encoder_m</code> est utilisé pour encoder la séquence de l'histoire (<code>input_sequence</code>) en une séquence de vecteurs denses de taille fixe. L'argument <code>output_dim</code> de cet encodeur est défini sur 64 dimensions pour créer une représentation dense de l'histoire.</p></li>\n",
    "  <li><p>L'encodeur <code>input_encoder_c</code> est utilisé pour encoder la séquence de l'histoire (<code>input_sequence</code>) en une séquence de vecteurs denses de taille <code>max_question_len</code>. L'argument <code>output_dim</code> de cet encodeur est défini sur <code>max_question_len</code> dimensions pour créer une représentation dense qui correspond à la question. Cela permet de tenir compte de la question dans le calcul de la réponse.</p></li>\n",
    "  <li><p>L'encodeur <code>question_encoder</code> est utilisé pour encoder la séquence de la question en une séquence de vecteurs denses. L'argument <code>output_dim</code> de cet encodeur est également défini sur 64 dimensions pour créer une représentation dense de la question. Cette représentation sera utilisée pour calculer la correspondance entre l'histoire et la question.</p></li>\n",
    "</ul>\n",
    "<p><b>Les trois encodeurs permettent de créer des représentations distinctes pour l'histoire, la question et leur correspondance, ce qui permet au modèle de tenir compte de la question lors de la prédiction de la réponse</b>.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size,output_dim=64))\n",
    "input_encoder_m.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size,output_dim=max_question_len))\n",
    "input_encoder_c.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=64,\n",
    "                               input_length=max_question_len))\n",
    "question_encoder.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd723248",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><code>input_encoded_m</code>: la sortie de l'encodeur <code>input_encoder_m</code>. Cette variable représente l'histoire encodée en une séquence de vecteurs denses de taille fixe.</li>\n",
    "    <li><code>input_encoded_c</code>: la sortie de l'encodeur <code>input_encoder_c</code>. Cette variable représente l'histoire encodée en une séquence de vecteurs denses de taille <code>max_question_len</code>.</li>\n",
    "    <li><code>question_encoded</code>: la sortie de l'encodeur <code>question_encoder</code>. Cette variable représente la question encodée en une séquence de vecteurs denses de taille fixe.</li>\n",
    "    <li><code>match</code>: la matrice de correspondance entre l'histoire et la question. Elle est calculée en prenant le produit scalaire entre les encodages de l'histoire et de la question. \n",
    "    <li><code>response</code>: la matrice résultante de l'addition de la matrice de correspondance et de l'encodeur <code>input_encoder_c</code>.)</li>\n",
    "    <li><code>answer</code>: la concaténation de la matrice de correspondance et de l'encodeur <code>question_encoder</code>..</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889e51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode input sequence and questions (which are indices)\n",
    "# to sequences of dense vectors\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)\n",
    "\n",
    "# shape: `(samples, story_maxlen, query_maxlen)`\n",
    "match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "match = Activation('softmax')(match)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# add the match matrix with the second input vector sequence\n",
    "response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n",
    "\n",
    "# concatenate the match matrix with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])\n",
    "\n",
    "# Regularization with Dropout\n",
    "answer = Dropout(0.5)(answer)\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "\n",
    "# we output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff83158",
   "metadata": {},
   "source": [
    "<p> On entraine le modèle </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "304d2f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "313/313 [==============================] - 4s 7ms/step - loss: 0.8935 - accuracy: 0.4949 - val_loss: 0.6945 - val_accuracy: 0.5030\n",
      "Epoch 2/120\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.7037 - accuracy: 0.5060 - val_loss: 0.6934 - val_accuracy: 0.5130\n",
      "Epoch 3/120\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.6987 - accuracy: 0.4922 - val_loss: 0.6950 - val_accuracy: 0.5030\n",
      "Epoch 4/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6966 - accuracy: 0.5028 - val_loss: 0.6940 - val_accuracy: 0.5030\n",
      "Epoch 5/120\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6964 - accuracy: 0.4919 - val_loss: 0.6938 - val_accuracy: 0.5030\n",
      "Epoch 6/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6958 - accuracy: 0.4946 - val_loss: 0.6932 - val_accuracy: 0.4980\n",
      "Epoch 7/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.6954 - accuracy: 0.5031 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 8/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6959 - accuracy: 0.4915 - val_loss: 0.6934 - val_accuracy: 0.5030\n",
      "Epoch 9/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.6948 - accuracy: 0.5010 - val_loss: 0.6935 - val_accuracy: 0.4970\n",
      "Epoch 10/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6954 - accuracy: 0.4912 - val_loss: 0.6933 - val_accuracy: 0.4970\n",
      "Epoch 11/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6952 - accuracy: 0.4968 - val_loss: 0.6940 - val_accuracy: 0.5030\n",
      "Epoch 12/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6956 - accuracy: 0.4985 - val_loss: 0.6935 - val_accuracy: 0.5030\n",
      "Epoch 13/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6952 - accuracy: 0.4968 - val_loss: 0.6933 - val_accuracy: 0.4970\n",
      "Epoch 14/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.6946 - accuracy: 0.5096 - val_loss: 0.6932 - val_accuracy: 0.4970\n",
      "Epoch 15/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6953 - accuracy: 0.4986 - val_loss: 0.6934 - val_accuracy: 0.5030\n",
      "Epoch 16/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.6948 - accuracy: 0.5018 - val_loss: 0.6934 - val_accuracy: 0.4970\n",
      "Epoch 17/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6954 - accuracy: 0.5033 - val_loss: 0.6936 - val_accuracy: 0.4970\n",
      "Epoch 18/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6946 - accuracy: 0.5025 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 19/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6958 - accuracy: 0.4949 - val_loss: 0.6932 - val_accuracy: 0.4970\n",
      "Epoch 20/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.6951 - accuracy: 0.4942 - val_loss: 0.6942 - val_accuracy: 0.5030\n",
      "Epoch 21/120\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.6946 - accuracy: 0.5040 - val_loss: 0.6933 - val_accuracy: 0.4970\n",
      "Epoch 22/120\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.6950 - accuracy: 0.4984 - val_loss: 0.6963 - val_accuracy: 0.5030\n",
      "Epoch 23/120\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.6946 - accuracy: 0.5052 - val_loss: 0.6949 - val_accuracy: 0.5030\n",
      "Epoch 24/120\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.6957 - accuracy: 0.4879 - val_loss: 0.6999 - val_accuracy: 0.4970\n",
      "Epoch 25/120\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.6951 - accuracy: 0.4976 - val_loss: 0.6937 - val_accuracy: 0.4970\n",
      "Epoch 26/120\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.6956 - accuracy: 0.4917 - val_loss: 0.6937 - val_accuracy: 0.5030\n",
      "Epoch 27/120\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.6950 - accuracy: 0.4982 - val_loss: 0.6933 - val_accuracy: 0.4970\n",
      "Epoch 28/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.6944 - accuracy: 0.5090 - val_loss: 0.6934 - val_accuracy: 0.4970\n",
      "Epoch 29/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6950 - accuracy: 0.5012 - val_loss: 0.6931 - val_accuracy: 0.5030\n",
      "Epoch 30/120\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6948 - accuracy: 0.5009 - val_loss: 0.6938 - val_accuracy: 0.4970\n",
      "Epoch 31/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.6954 - accuracy: 0.4911 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 32/120\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.6952 - accuracy: 0.4939 - val_loss: 0.6951 - val_accuracy: 0.4970\n",
      "Epoch 33/120\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6947 - accuracy: 0.4980 - val_loss: 0.6935 - val_accuracy: 0.5030\n",
      "Epoch 34/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6949 - accuracy: 0.5002 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 35/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.6952 - accuracy: 0.4926 - val_loss: 0.6958 - val_accuracy: 0.5030\n",
      "Epoch 36/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.6945 - accuracy: 0.5054 - val_loss: 0.6937 - val_accuracy: 0.4970\n",
      "Epoch 37/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6953 - accuracy: 0.4957 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 38/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6950 - accuracy: 0.5038 - val_loss: 0.6936 - val_accuracy: 0.5030\n",
      "Epoch 39/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6948 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4970\n",
      "Epoch 40/120\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6950 - accuracy: 0.4964 - val_loss: 0.6931 - val_accuracy: 0.5030\n",
      "Epoch 41/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.6942 - accuracy: 0.5059 - val_loss: 0.6939 - val_accuracy: 0.5030\n",
      "Epoch 42/120\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6952 - accuracy: 0.4995 - val_loss: 0.6956 - val_accuracy: 0.4970\n",
      "Epoch 43/120\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6952 - accuracy: 0.4998 - val_loss: 0.6959 - val_accuracy: 0.4970\n",
      "Epoch 44/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6951 - accuracy: 0.4955 - val_loss: 0.6954 - val_accuracy: 0.4970\n",
      "Epoch 45/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6945 - accuracy: 0.5008 - val_loss: 0.6931 - val_accuracy: 0.5030\n",
      "Epoch 46/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6946 - accuracy: 0.5004 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 47/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6953 - accuracy: 0.4936 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 48/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6945 - accuracy: 0.5020 - val_loss: 0.6933 - val_accuracy: 0.5030\n",
      "Epoch 49/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.6949 - accuracy: 0.5003 - val_loss: 0.6933 - val_accuracy: 0.5030\n",
      "Epoch 50/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.6951 - accuracy: 0.4942 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 51/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6945 - accuracy: 0.4983 - val_loss: 0.6936 - val_accuracy: 0.4970\n",
      "Epoch 52/120\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.6953 - accuracy: 0.4927 - val_loss: 0.6930 - val_accuracy: 0.4860\n",
      "Epoch 53/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6943 - accuracy: 0.5010 - val_loss: 0.6936 - val_accuracy: 0.5040\n",
      "Epoch 54/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6939 - accuracy: 0.5120 - val_loss: 0.6935 - val_accuracy: 0.4990\n",
      "Epoch 55/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.6935 - accuracy: 0.5138 - val_loss: 0.6938 - val_accuracy: 0.4770\n",
      "Epoch 56/120\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.6935 - accuracy: 0.5146 - val_loss: 0.6937 - val_accuracy: 0.5080\n",
      "Epoch 57/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 11ms/step - loss: 0.6928 - accuracy: 0.5187 - val_loss: 0.6943 - val_accuracy: 0.4980\n",
      "Epoch 58/120\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.6922 - accuracy: 0.5211 - val_loss: 0.6950 - val_accuracy: 0.5000\n",
      "Epoch 59/120\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.6919 - accuracy: 0.5255 - val_loss: 0.6933 - val_accuracy: 0.4950\n",
      "Epoch 60/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6894 - accuracy: 0.5297 - val_loss: 0.6926 - val_accuracy: 0.4970\n",
      "Epoch 61/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6877 - accuracy: 0.5321 - val_loss: 0.6912 - val_accuracy: 0.5030\n",
      "Epoch 62/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6862 - accuracy: 0.5406 - val_loss: 0.6861 - val_accuracy: 0.5290\n",
      "Epoch 63/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.6790 - accuracy: 0.5533 - val_loss: 0.6791 - val_accuracy: 0.5510\n",
      "Epoch 64/120\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6667 - accuracy: 0.5814 - val_loss: 0.6633 - val_accuracy: 0.6070\n",
      "Epoch 65/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6606 - accuracy: 0.5881 - val_loss: 0.6558 - val_accuracy: 0.6190\n",
      "Epoch 66/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6487 - accuracy: 0.6111 - val_loss: 0.6464 - val_accuracy: 0.6270\n",
      "Epoch 67/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6452 - accuracy: 0.6201 - val_loss: 0.6299 - val_accuracy: 0.6560\n",
      "Epoch 68/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6367 - accuracy: 0.6345 - val_loss: 0.6282 - val_accuracy: 0.6400\n",
      "Epoch 69/120\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.6309 - accuracy: 0.6345 - val_loss: 0.6167 - val_accuracy: 0.6640\n",
      "Epoch 70/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6181 - accuracy: 0.6541 - val_loss: 0.6073 - val_accuracy: 0.6620\n",
      "Epoch 71/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.6109 - accuracy: 0.6611 - val_loss: 0.6042 - val_accuracy: 0.6540\n",
      "Epoch 72/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.6052 - accuracy: 0.6664 - val_loss: 0.5936 - val_accuracy: 0.6640\n",
      "Epoch 73/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.5917 - accuracy: 0.6828 - val_loss: 0.5776 - val_accuracy: 0.6900\n",
      "Epoch 74/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.5873 - accuracy: 0.6915 - val_loss: 0.5785 - val_accuracy: 0.6810\n",
      "Epoch 75/120\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.5784 - accuracy: 0.7016 - val_loss: 0.5693 - val_accuracy: 0.6870\n",
      "Epoch 76/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.5737 - accuracy: 0.7017 - val_loss: 0.5694 - val_accuracy: 0.6920\n",
      "Epoch 77/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.5609 - accuracy: 0.7149 - val_loss: 0.5561 - val_accuracy: 0.7010\n",
      "Epoch 78/120\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.5559 - accuracy: 0.7198 - val_loss: 0.5523 - val_accuracy: 0.7100\n",
      "Epoch 79/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.5524 - accuracy: 0.7281 - val_loss: 0.5430 - val_accuracy: 0.7260\n",
      "Epoch 80/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.5398 - accuracy: 0.7355 - val_loss: 0.5326 - val_accuracy: 0.7260\n",
      "Epoch 81/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.5282 - accuracy: 0.7459 - val_loss: 0.5202 - val_accuracy: 0.7340\n",
      "Epoch 82/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.5222 - accuracy: 0.7474 - val_loss: 0.5135 - val_accuracy: 0.7450\n",
      "Epoch 83/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.5147 - accuracy: 0.7566 - val_loss: 0.5132 - val_accuracy: 0.7490\n",
      "Epoch 84/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.5050 - accuracy: 0.7615 - val_loss: 0.5146 - val_accuracy: 0.7470\n",
      "Epoch 85/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.4942 - accuracy: 0.7723 - val_loss: 0.5014 - val_accuracy: 0.7520\n",
      "Epoch 86/120\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.4861 - accuracy: 0.7724 - val_loss: 0.4759 - val_accuracy: 0.7590\n",
      "Epoch 87/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.4821 - accuracy: 0.7731 - val_loss: 0.4728 - val_accuracy: 0.7760\n",
      "Epoch 88/120\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.4725 - accuracy: 0.7849 - val_loss: 0.4755 - val_accuracy: 0.7690\n",
      "Epoch 89/120\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.4669 - accuracy: 0.7840 - val_loss: 0.4542 - val_accuracy: 0.7820\n",
      "Epoch 90/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.4591 - accuracy: 0.7898 - val_loss: 0.4632 - val_accuracy: 0.7700\n",
      "Epoch 91/120\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.4525 - accuracy: 0.7888 - val_loss: 0.4620 - val_accuracy: 0.7760\n",
      "Epoch 92/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.4456 - accuracy: 0.7984 - val_loss: 0.4536 - val_accuracy: 0.7740\n",
      "Epoch 93/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.4478 - accuracy: 0.7943 - val_loss: 0.4705 - val_accuracy: 0.7760\n",
      "Epoch 94/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.4396 - accuracy: 0.7985 - val_loss: 0.4500 - val_accuracy: 0.7790\n",
      "Epoch 95/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.4368 - accuracy: 0.8009 - val_loss: 0.4302 - val_accuracy: 0.7800\n",
      "Epoch 96/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.4298 - accuracy: 0.8040 - val_loss: 0.4439 - val_accuracy: 0.7820\n",
      "Epoch 97/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.4273 - accuracy: 0.8056 - val_loss: 0.4487 - val_accuracy: 0.7830\n",
      "Epoch 98/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.4196 - accuracy: 0.8066 - val_loss: 0.4497 - val_accuracy: 0.7790\n",
      "Epoch 99/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.4218 - accuracy: 0.8085 - val_loss: 0.4427 - val_accuracy: 0.7860\n",
      "Epoch 100/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.4213 - accuracy: 0.8070 - val_loss: 0.4319 - val_accuracy: 0.7800\n",
      "Epoch 101/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.4194 - accuracy: 0.8115 - val_loss: 0.4396 - val_accuracy: 0.7850\n",
      "Epoch 102/120\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.4096 - accuracy: 0.8133 - val_loss: 0.4421 - val_accuracy: 0.7880\n",
      "Epoch 103/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.4133 - accuracy: 0.8118 - val_loss: 0.4438 - val_accuracy: 0.7800\n",
      "Epoch 104/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.4031 - accuracy: 0.8145 - val_loss: 0.4463 - val_accuracy: 0.7830\n",
      "Epoch 105/120\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.4012 - accuracy: 0.8181 - val_loss: 0.4904 - val_accuracy: 0.7800\n",
      "Epoch 106/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.4077 - accuracy: 0.8158 - val_loss: 0.4327 - val_accuracy: 0.7870\n",
      "Epoch 107/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.4071 - accuracy: 0.8154 - val_loss: 0.4416 - val_accuracy: 0.7790\n",
      "Epoch 108/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.3997 - accuracy: 0.8177 - val_loss: 0.4339 - val_accuracy: 0.7830\n",
      "Epoch 109/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.3939 - accuracy: 0.8191 - val_loss: 0.4582 - val_accuracy: 0.7830\n",
      "Epoch 110/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.3959 - accuracy: 0.8204 - val_loss: 0.4563 - val_accuracy: 0.7800\n",
      "Epoch 111/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.3960 - accuracy: 0.8205 - val_loss: 0.4329 - val_accuracy: 0.7860\n",
      "Epoch 112/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.3916 - accuracy: 0.8220 - val_loss: 0.4450 - val_accuracy: 0.7890\n",
      "Epoch 113/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 11ms/step - loss: 0.3871 - accuracy: 0.8220 - val_loss: 0.4326 - val_accuracy: 0.7880\n",
      "Epoch 114/120\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 0.3891 - accuracy: 0.8243 - val_loss: 0.4229 - val_accuracy: 0.7860\n",
      "Epoch 115/120\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.3840 - accuracy: 0.8252 - val_loss: 0.4318 - val_accuracy: 0.7900\n",
      "Epoch 116/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.3861 - accuracy: 0.8226 - val_loss: 0.4282 - val_accuracy: 0.7940\n",
      "Epoch 117/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.3865 - accuracy: 0.8190 - val_loss: 0.4417 - val_accuracy: 0.7790\n",
      "Epoch 118/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.3829 - accuracy: 0.8258 - val_loss: 0.4335 - val_accuracy: 0.7900\n",
      "Epoch 119/120\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.3837 - accuracy: 0.8284 - val_loss: 0.4311 - val_accuracy: 0.7980\n",
      "Epoch 120/120\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.3775 - accuracy: 0.8273 - val_loss: 0.4365 - val_accuracy: 0.7860\n"
     ]
    }
   ],
   "source": [
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit([inputs_train, queries_train], answers_train,batch_size=32,epochs=120,validation_data=([inputs_test, queries_test], answers_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ed9b6b",
   "metadata": {},
   "source": [
    "<p> On sauvgarde le modèle </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5725d369",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'chatbot.h5'\n",
    "model.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef7b913",
   "metadata": {},
   "source": [
    "<p> On importe le modèle sauvgarder et on fait une prédiction </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e027f93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(filename)\n",
    "pred_results = model.predict(([inputs_test, queries_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "18cd3008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daniel went back to the kitchen . Mary grabbed the apple there . Daniel journeyed to the office . John went back to the office .\n"
     ]
    }
   ],
   "source": [
    "story =' '.join(word for word in test_data[6][0])\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9aea20c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Daniel in the hallway ?\n"
     ]
    }
   ],
   "source": [
    "query = ' '.join(word for word in test_data[6][1])\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "63bbec3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Test Answer from Data is: no\n"
     ]
    }
   ],
   "source": [
    "print(\"True Test Answer from Data is:\",test_data[6][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "27668be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.44307885e-10, 1.06817936e-10, 6.75345543e-11, 2.98000896e-06,\n",
       "       8.78935258e-11, 9.70785050e-11, 9.26916391e-11, 1.03653995e-10,\n",
       "       1.13779125e-10, 1.04337330e-10, 9.79919271e-11, 9.99997020e-01,\n",
       "       8.81276996e-11, 1.22476709e-10, 7.50097345e-11, 1.31800959e-10,\n",
       "       1.20896140e-10, 9.41382111e-11, 9.57071575e-11, 9.11585599e-11,\n",
       "       1.04153214e-10, 7.88830390e-11, 1.11560990e-10, 9.43037246e-11,\n",
       "       9.60505980e-11, 1.07073322e-10, 1.40758932e-10, 9.73044839e-11,\n",
       "       5.60347949e-11, 9.40531403e-11, 9.44796186e-11, 9.19859189e-11,\n",
       "       1.03219176e-10, 8.92293045e-11, 1.37223941e-10, 8.93181917e-11,\n",
       "       1.15663951e-10, 1.20860863e-10], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6b07b4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer is:  no\n",
      "Probability of certainty was:  0.999997\n"
     ]
    }
   ],
   "source": [
    "#Generate prediction from model\n",
    "val_max = np.argmax(pred_results[6])\n",
    "\n",
    "for key, val in tokenizer.word_index.items():\n",
    "    if val == val_max:\n",
    "        k = key\n",
    "\n",
    "print(\"Predicted answer is: \", k)\n",
    "print(\"Probability of certainty was: \", pred_results[6][val_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc49935",
   "metadata": {},
   "source": [
    "<p> On créé notre propre histoire et question. Attention, il faut utiliser seulement les mots  qu'il y a dans <code>vocabs</code> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fe71fe87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John',\n",
       " 'left',\n",
       " 'the',\n",
       " 'kitchen',\n",
       " '.',\n",
       " 'Sandra',\n",
       " 'dropped',\n",
       " 'the',\n",
       " 'football',\n",
       " 'in',\n",
       " 'the',\n",
       " 'garden',\n",
       " '.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the whitespace of the periods\n",
    "my_story = \"Erika left the bathroom . John dropped the football in the garden .\"\n",
    "my_story.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ca52d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_question = \"Is the football in the garden ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c453f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = [(my_story.split(),my_question.split(),'yes')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e4caf053",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_story,my_ques,my_ans = vectorize_stories(mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8725b277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_results = model.predict(([ my_story, my_ques]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4791b675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer is:  yes\n",
      "Probability of certainty was:  0.9977841\n"
     ]
    }
   ],
   "source": [
    "#Generate prediction from model\n",
    "val_max = np.argmax(pred_results[0])\n",
    "\n",
    "for key, val in tokenizer.word_index.items():\n",
    "    if val == val_max:\n",
    "        k = key\n",
    "\n",
    "print(\"Predicted answer is: \", k)\n",
    "print(\"Probability of certainty was: \", pred_results[0][val_max])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
